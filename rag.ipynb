{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ea3cbc",
   "metadata": {},
   "source": [
    "# Setting environment variables to log traces with Langsmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40728a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os          # Imports Python's built-in \"os module\" for interacting with the operating system. (e.g environment variables)\n",
    "from dotenv import load_dotenv          # Imports the \"load_dotenv function\" from the \"dotenv module\" to load environment variables from the .env file. \n",
    "import requests          # Imports the \"requests library\" to make HTTP requests (used to verify the Langsmith API connection)\n",
    "\n",
    "\n",
    "load_dotenv(          # Loads environment variables from the .env file.\n",
    "    dotenv_path = \".env\",           #Specifies the path to the .env file, which contains environment variables. The default is .env in the current directory.\n",
    "    override = True           # Allows the loaded environment variables in the .env file to override any existing environment variables. \n",
    ")\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")          # Retrieves the value of the loaded \"LANGSMITH_TRACING\" variable from the .env file and sets it in Python's runtime environment. This ensures Langsmith can access the value.\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")          # Retrieves the value of the loaded \"LANGSMITH_ENDPOINT\" variable from the .env file and sets it in Python's runtime environment. This ensures Langsmith can access the value.\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")          # Retrieves the value of the loaded \"LANGSMITH_API_KEY\" variable from the .env file and sets it in Python's runtime environment. This ensures Langsmith can access the value.\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")          # Retrieves the value of the loaded \"LANGSMITH_PROJECT\" variable from the .env file and sets it in Python's runtime environment. This ensures Langsmith can access the value.\n",
    "\n",
    "headers = {          # Creates a dictionary to store HTTP headers for the request. This particular dictionary is for the \"Authorization header\" which is required to aunthenticate the request to the Langsmith API.\n",
    "    \"Authorization\": f\"Bearer {os.getenv(\"LANGSMITH_API_KEY\")}\"          # Sets the \"Authorization header\" with the value of the \"LANGSMITH_API_KEY\" environment variable. This is used to authenticate the request to the Langsmith API.\n",
    "}\n",
    "response = requests.get(          # Makes a GET request to Langsmith's API endpoint to verify the connection.\n",
    "    \"https://api.smith.langchain.com\",          # The URL of Langsmith's API endpoint.\n",
    "    headers=headers          # Passes the dictionary containing the \"Authorization header\" to authenticate the request.\n",
    ")\n",
    "\n",
    "print(response.status_code)          # Prints the HTTP status code of the response. A status code of 200 indicates a successful connection to the Langsmith API.\n",
    "print(response.json())          # Prints the JSON response from the Langsmith API, which typically contains information about the API connection or any relevant data returned by the request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05bb6b",
   "metadata": {},
   "source": [
    "#                               INDEXING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f50df",
   "metadata": {},
   "source": [
    "# Loading the PDF document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader          # Imports the \"PyMuPDFLoader\" class from LangChain's document loaders. This loader specializes in extracting text and metadata from PDF files using the PyMuPDF library.\n",
    "import pprint          # Imports the \"pprint module\" for pretty-printing data structures, making them easier to read in the console.\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\user\\Downloads\\HANNY ABUBAKAR CV.pdf\"          # Specifies the path to the PDF file that wiill be loaded. The \"r\" prefix ensures that the backslashes are treated as \"literal characters\" and not as escape sequences. \n",
    "loader = PyMuPDFLoader(          # Instantiates and Initializes the \"PyMuPDFLoader\" with the specified PDF file path. \n",
    "    file_path,          # The path of the file to be loaded.\n",
    "    # mode=\"single\"          # Specfies the mode in which the document will be loaded. The \"single\" mode means the entire document will be treated as one, the \"page\" mode means that each page will be treated as a separate document. \n",
    ")          \n",
    "loaded_doc = loader.load()          # Executes the PDF parsing and text extraction process, returning a list of Document objects.     \n",
    "\n",
    "print(f\"This document has {len(loaded_doc)} pages.\")\n",
    "pprint.pp(loaded_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955531b",
   "metadata": {},
   "source": [
    "# Splitting the loaded PDF document into chunks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter          # Imports the \"RecursiveCharacterTextSplitter\" class from Langchain's text splitters. This class attempts to keep larger units (e.g., paragraphs or sentences) intact while keeping the text within a specified character limit. \n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(          # Instantiates and Initializes the \"RecursiveCharacterTextSplitter\" with specific paramaters on how to split the text.\n",
    "    chunk_size = 1000,          # Defines the maximum number of characters in each chunk. (the text will be split into chunks that are at most 1000 characters long).\n",
    "    chunk_overlap = 200,          # Defines the maximum number of characters that can overlap between consecutive chunks. \n",
    ")\n",
    "all_chunks = text_splitter.split_documents(loaded_doc)          # Splits the loaded PDF document into chunks. Each chunk will be a Document object.\n",
    "\n",
    "print(f\"This document has been split into {len(all_chunks)} chunks.\")          \n",
    "for each_chunk in all_chunks:          \n",
    "    print(\"\")\n",
    "    print(each_chunk.page_content)\n",
    "    print(\"\")\n",
    "    print(\"-----\" * 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236d3e6",
   "metadata": {},
   "source": [
    "# Embedding the chunks as vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ad184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings          # Imports the \"OllamaEmbeddings\" class from Langchain's Ollama module, in order to embed the chunks as vectors.\n",
    "\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model = \"nomic-embed-text\")          # Instantiates and Initializes the \"OllamaEmbeddings\" model.\n",
    "\n",
    "all_chunks_texts = [each_chunk.page_content for each_chunk in all_chunks]          # Extracts the content of each chunk from all the chunks using a \"list comprehension\", and saves it to a list. This creates a list of strings, where each string is the content of a chunk. \n",
    "all_chunks_vectors = embedding_model.embed_documents(all_chunks_texts)          # Embeds the content of all chunks as vectors using the \"OllamaEmbeddings\" model. This converts the text into numerical representations (vectors) that can be used for similarity search.\n",
    "\n",
    "for each_chunk_vector in all_chunks_vectors:          \n",
    "    print(each_chunk_vector)\n",
    "    print(\"\")\n",
    "    print(\"-----\" * 25000)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befa865",
   "metadata": {},
   "source": [
    "# Creating a Vector Store and Storing the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss          # Imports the \"faiss library\", which is used for similarity search and clustering of dense vectors.\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore          # Imports the \"InMemoryDocstore\" class from Langchain's community module, which provides an in-memory non-persistent document store for text/metadata storage.\n",
    "from langchain_community.vectorstores import FAISS          # Imports the \"FAISS\" class from Langchain's community vector stores, which provides an interface to work with FAISS.\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embedding_model.embed_query(\"Hello World\")))          # Creates an \"IndexFlatL2\" FAISS index, which stores vectors and uses L2 distance (Euclidean distance) for similarity search. It's dimensionality is set to length of the vectors produced by the embedding model. \n",
    "\n",
    "vector_store = FAISS(          # Instantiates the FAISS vector store with the specified parameters.\n",
    "    embedding_function = embedding_model,          # Sets the embedding model to be used to generate embeddings. \n",
    "    index = index,          # Sets the FAISS index created earlier to be used for storing and searching vectors.\n",
    "    docstore = InMemoryDocstore(),          # Initializes an in-memory document store for metadata/text. \n",
    "    index_to_docstore_id = {},          # Initializes an empty dictionary to map index IDs to document store IDs, in order to keep track of which document corresponds to which vector in the index. \n",
    ")\n",
    "\n",
    "vector_store.add_texts(          # Adds the texts, vectors, and metadata to the vector store.\n",
    "    texts = all_chunks_texts,          # Adds the texts.\n",
    "    vectors = all_chunks_vectors,          # Adds the vectors (to skip re-embedding).\n",
    "    metadatas = [{\"Source\": f\"chunk_{i}\"} for i in range(len(all_chunks_texts))],          # Adds metadata to each chunk (for traceability).\n",
    ")\n",
    "\n",
    "vector_store.save_local(\"Vector_Store\")          # Saves the vector store to the local directory named \"Vector_Store\".\n",
    "\n",
    "new_vector_store = FAISS.load_local(          # Initializes a new FAISS vector store by loading it with the specified parameters.  \n",
    "    \"Vector_Store\",          # Sets the path to the initial vector store. (The initial vector store will be part of the new vector store)\n",
    "    embedding_model,          # Sets the embedding model to be used to generate embeddings. \n",
    "    allow_dangerous_deserialization = True,          # Allows deserialization of potentially unsafe data and is useful when loading vector stores.\n",
    ")\n",
    "\n",
    "query = \"What was Hanny's education history?\"          # Defines the search query to retrieve relevant documents.\n",
    "\n",
    "chunk_retriever = new_vector_store.as_retriever(          # Converts the vector store into a LangChain retriever.\n",
    "    search_type = \"mmr\",          # Sets the search type to \"mmr\" (Maximum Marginal Relevane), which balances relevance with diversity in the retrieved results.\n",
    "    search_kwargs = {\"k\":2}           # Retrieves the top 2 most relevant chunks based on the query.\n",
    ")\n",
    "\n",
    "all_retrieved_chunks = chunk_retriever.invoke(query)          # Retrieves the relevant chunks based on the query, resulting in a list of Document objects.\n",
    "\n",
    "context = \"\\n\\n\".join([each_retrieved_chunk.page_content for each_retrieved_chunk in all_retrieved_chunks])          # Extracts text from each retrieved chunk and joins them into a single string, separated by double newlines. \n",
    "\n",
    "print(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
